# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12owjcKhh6R-ZWcKykUw2aOPLlmr9C5h3

**Import Libraries**
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import matplotlib.pyplot as plt
import pickle

"""**Download NLTK resources**"""

nltk.download('stopwords')
nltk.download('wordnet')

"""**Load the dataset**"""

file_path = '/content/test.csv'  # Replace with your file path
df = pd.read_csv(file_path)
df = df.dropna(subset=['crimeaditionalinfo', 'category'])  # Drop rows with missing values

"""**Preprocess the text**


"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()  # Lowercase
    tokens = [word for word in text.split() if word not in stop_words]  # Remove stopwords
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize
    return ' '.join(tokens)

df['processed_text'] = df['crimeaditionalinfo'].apply(preprocess_text)

"""**Encode labels**"""

label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['category'])
num_labels = df['label'].nunique()

# Save the label encoder for later use
with open('label_encoder.pkl', 'wb') as le_file:
    pickle.dump(label_encoder, le_file)

"""**Split dataset**"""

X_train, X_val, y_train, y_val = train_test_split(
    df['processed_text'], df['label'], test_size=0.2, random_state=42
)

"""**Tokenize and pad sequences**"""

max_vocab_size = 5000
max_sequence_length = 100

tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_sequence_length, padding='post')

# Save the tokenizer for later use
with open('tokenizer.pkl', 'wb') as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sawarn69/glove6b100dtxt")

print("Path to dataset files:", path)

"""**Load Pretrained Embeddings (Using GloVe)**"""

import os

# Check the contents of the directory
glove_dir = '/root/.cache/kagglehub/datasets/sawarn69/glove6b100dtxt/versions/1'
print(os.listdir(glove_dir))

def load_glove_embeddings(embedding_dim=100):
    embeddings_index = {}
    glove_file_path = '/root/.cache/kagglehub/datasets/sawarn69/glove6b100dtxt/versions/1/glove.6B.100d.txt'
    with open(glove_file_path, encoding="utf-8") as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

embedding_dim = 100
embeddings_index = load_glove_embeddings(embedding_dim)

# Create embedding matrix
word_index = tokenizer.word_index
embedding_matrix = np.zeros((max_vocab_size, embedding_dim))
for word, i in word_index.items():
    if i < max_vocab_size:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

"""**Build the BiLSTM model**"""

model = Sequential([
    Embedding(
        input_dim=max_vocab_size,
        output_dim=embedding_dim,
        input_length=max_sequence_length,
        weights=[embedding_matrix],
        trainable=True  # Fine-tune embeddings
    ),
    Bidirectional(LSTM(64, return_sequences=True)),  # Bidirectional LSTM
    Dropout(0.3),  # Regularization
    Bidirectional(LSTM(64)),  # Second LSTM layer
    Dense(64, activation='relu'),  # Fully connected layer
    Dropout(0.3),  # Dropout
    Dense(num_labels, activation='softmax')  # Output layer
])

# Build the model by running a dummy input
dummy_input = np.random.randint(0, max_vocab_size, size=(1, max_sequence_length))
model.predict(dummy_input)  # This will build the model

# View the model summary
model.summary()

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

"""**Train the model**"""

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    X_train_pad, y_train,
    validation_data=(X_val_pad, y_val),
    epochs=5,  # Number of epochs can be adjusted
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

"""**Evaluate the model**"""

val_loss, val_accuracy = model.evaluate(X_val_pad, y_val)
print(f"Validation Accuracy: {val_accuracy:.2f}")

"""**Predict and compute metrics**"""

from sklearn.metrics import classification_report, accuracy_score
import numpy as np
y_val_pred = model.predict(X_val_pad)
# If y_val contains class labels, and your model outputs probabilities:
y_val_pred = np.argmax(y_val_pred, axis=1)
# Ensure num_labels matches the actual number of classes in y_val
num_labels = len(label_encoder.classes_)  # This should give you 14 if you have 14 classes in y_val

# Ensure that target_names matches the number of classes in y_val
report = classification_report(y_val, y_val_pred, target_names=label_encoder.classes_[:len(np.unique(y_val))],
                              output_dict=True, zero_division=0)

# Print overall precision, recall, and f1-score (macro averages)
precision = report['macro avg']['precision']
recall = report['macro avg']['recall']
f1_score = report['macro avg']['f1-score']

print("\nOverall Precision (Macro avg):", precision)
print("Overall Recall (Macro avg):", recall)
print("Overall F1-Score (Macro avg):", f1_score)

"""**Plot training history**"""

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training History')
plt.legend()
plt.show()

"""**Save the model**"""

model.save('bilstm1_model.keras')